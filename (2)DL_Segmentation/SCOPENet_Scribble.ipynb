{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58654a37",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# ======================\n",
    "# MONAI (medical imaging AI)\n",
    "# ======================\n",
    "from monai.utils import set_determinism, first              # Reproducibility + utility\n",
    "from monai.transforms import (                              # Preprocessing & augmentation transforms\n",
    "    EnsureChannelFirstD, Compose, LoadImageD,\n",
    "    RandRotateD, RandZoomD, ScaleIntensityRanged\n",
    ")\n",
    "from monai.data import DataLoader, Dataset, CacheDataset    # Data handling classes\n",
    "from monai.config import print_config, USE_COMPILED         # Config info\n",
    "from monai.networks.nets import *                           # Neural net architectures\n",
    "from monai.networks.blocks import Warp                      # Warping layer\n",
    "from monai.apps import MedNISTDataset                       # Example dataset loader\n",
    "from monai.losses import *                                  # MONAI loss functions\n",
    "from monai.metrics import *                                 # MONAI evaluation metrics\n",
    "\n",
    "# ======================\n",
    "# PyTorch core\n",
    "# ======================\n",
    "import torch                                                 # Core PyTorch\n",
    "import torch.nn as nn                                        # Neural network layers\n",
    "import torch.nn.functional as F                              # Functional API\n",
    "import torch.optim as optim                                  # Optimizers\n",
    "from torch.nn import MSELoss                                 # MSE loss (regression)\n",
    "from torch.autograd import Variable                          # Autograd wrapper (legacy use)\n",
    "from torch.utils.data import Sampler, DataLoader             # Sampling & batching utilities\n",
    "from torch.utils.tensorboard import SummaryWriter            # TensorBoard logging\n",
    "from torch.cuda.amp import GradScaler, autocast              # Mixed-precision training\n",
    "\n",
    "# ======================\n",
    "# PyTorch model inspection / visualization\n",
    "# ======================\n",
    "from torchinfo import summary                                # Model summary (layers/params/shapes)\n",
    "from torchviz import make_dot, make_dot_from_trace           # Computation graph visualization\n",
    "from fvcore.nn import FlopCountAnalysis                      # FLOPs counting utility\n",
    "\n",
    "# ======================\n",
    "# Computer vision & augmentation\n",
    "# ======================\n",
    "import cv2                                                   # OpenCV for image processing\n",
    "import albumentations as A                                   # Augmentation framework\n",
    "from albumentations.pytorch import ToTensorV2                # Albumentations â†’ PyTorch tensor\n",
    "\n",
    "# ======================\n",
    "# Scientific / numerical stack\n",
    "# ======================\n",
    "import numpy as np                                            # Numerical operations\n",
    "import pandas as pd                                           # DataFrames / CSV handling\n",
    "from scipy.ndimage import label, gaussian_filter1d            # Morphology & smoothing\n",
    "from scipy.spatial.distance import directed_hausdorff         # Distance metric (e.g., surface matching)\n",
    "\n",
    "# ======================\n",
    "# Metrics & analysis\n",
    "# ======================\n",
    "import torchmetrics                                           # Metrics (e.g., Dice, IoU, etc.)\n",
    "from piqa import SSIM                                         # Structural Similarity metric\n",
    "\n",
    "# ======================\n",
    "# Visualization\n",
    "# ======================\n",
    "import matplotlib.pyplot as plt                               # Plotting\n",
    "import visdom                                                  # Real-time experiment dashboard\n",
    "from tqdm import tqdm                                          # Progress bar\n",
    "\n",
    "# ======================\n",
    "# I/O and file handling\n",
    "# ======================\n",
    "from glob import glob                                          # File matching patterns\n",
    "import nibabel as nib                                          # NIfTI medical image I/O\n",
    "from openpyxl import load_workbook, Workbook                   # Excel file handling\n",
    "import os                                                      # File system ops\n",
    "import tempfile                                                # Temporary files\n",
    "import re                                                      # Regex\n",
    "\n",
    "# ======================\n",
    "# Utilities\n",
    "# ======================\n",
    "import random                                                  # Random number generation\n",
    "from collections import defaultdict                           # Dict with default factory\n",
    "import config                                                  # Local config module\n",
    "from helper import *                                           # Local helper functions\n",
    "\n",
    "# ======================\n",
    "# Runtime info\n",
    "# ======================\n",
    "print_config()                                                 # Show MONAI + env config\n",
    "set_determinism(42)                                            # Set RNG seed for reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7835f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mask image in grayscale mode (0-255 intensity values)\n",
    "mask = cv2.imread(\"Patient001_slice001time001.png\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Convert white pixels (255) to label 1\n",
    "#   - This assumes binary mask format: background=0, foreground=255\n",
    "mask[mask == 255] = 1\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# Parameters for generate_scribble_segments():\n",
    "# mask (np.ndarray)          : 2D binary mask input (values 0/1)\n",
    "# target_label (int)         : Label value in the mask to generate scribble for\n",
    "# segment_num (Tuple[int,int]): Range of number of path segments to split the skeleton into\n",
    "# wave_num (Tuple[int,int])  : Range of sinusoidal wave counts per segment\n",
    "# amp_frac (Tuple[float,float]): Amplitude factor range (scaled by distance transform)\n",
    "# max_hide (int)             : Maximum number of segments to randomly omit\n",
    "# resample_step (int)        : Path resampling step size in pixels\n",
    "# min_amp_px (float)         : Minimum absolute offset threshold (in pixels)\n",
    "# smooth_sigma (float)       : Gaussian smoothing sigma for final path\n",
    "# line_width (int or Tuple[int,int]): Fixed or random stroke thickness in pixels\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "plt.figure(figsize=(20, 10))  # Create figure for visualizing multiple runs\n",
    "\n",
    "# Generate and visualize 20 random scribble variants\n",
    "for i in range(20):\n",
    "    scribble = generate_scribble_segments(\n",
    "        mask,\n",
    "        target_label=200,\n",
    "        segment_num=(1, 3),\n",
    "        wave_num=(1, 2),\n",
    "        amp_frac=(0.1, 1.8),\n",
    "        max_hide=1,\n",
    "        resample_step=2,\n",
    "        min_amp_px=1.0,\n",
    "        smooth_sigma=3.0,\n",
    "        line_width=(3, 10)\n",
    "    )\n",
    "\n",
    "    # Plot each scribble in a 4x5 grid\n",
    "    plt.subplot(4, 5, i + 1)\n",
    "    plt.imshow(scribble, cmap='gray')        # Show binary scribble mask\n",
    "    plt.title(f'Run {i+1}')                  # Indicate run number\n",
    "    plt.axis('off')                          # Hide axes for cleaner display\n",
    "\n",
    "plt.tight_layout()                           # Adjust spacing between subplots\n",
    "plt.show()                                   # Display the figure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8094c459",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Print how many GPUs are visible to PyTorch\n",
    "print('How many GPUs = ' + str(torch.cuda.device_count()))\n",
    "\n",
    "# Select device: first GPU if available, otherwise fallback to CPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)  # Show which device is selected\n",
    "\n",
    "# If no GPU is available, stop execution (CPU training would be too slow)\n",
    "if not torch.cuda.is_available():\n",
    "    raise Exception(\"GPU not available. CPU training will be too slow.\")\n",
    "\n",
    "# Print the name/model of the first GPU\n",
    "print(\"device name\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a80dee8",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Name for saving model checkpoints and outputs\n",
    "saveFile = 'SCOPENet_Scribble'\n",
    "\n",
    "# Path to the dataset directory (radial echocardiography data)\n",
    "data_dir = '/home/dario/KAMRUL/Radial4D_Paper/data/FETAL_radial/'\n",
    "print(data_dir)  # Print dataset path for confirmation\n",
    "\n",
    "# Full path/filename for saving the trained model checkpoint\n",
    "checkpoint_path = saveFile + '.pth'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd31f7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# Data Augmentation Pipelines (Albumentations)\n",
    "# ======================\n",
    "\n",
    "# --- Geometric augmentations ---\n",
    "geo_aug = A.OneOf([  # Randomly choose ONE of the following transformations\n",
    "    A.HorizontalFlip(p=0.1),         # Flip horizontally with 10% probability\n",
    "    A.Rotate(limit=10, p=1.0)        # Rotate randomly within Â±10 degrees\n",
    "], p=0.6)                            # Apply one of them with 60% probability overall\n",
    "\n",
    "# --- Photometric augmentations ---\n",
    "photo_aug = A.SomeOf([  # Randomly choose a fixed number (n=1) of these augmentations\n",
    "    A.MultiplicativeNoise(           # Apply multiplicative Gaussian noise\n",
    "        multiplier=(0.8, 1.2),       # Scale factor range per pixel\n",
    "        per_channel=True,            # Different noise per channel\n",
    "        elementwise=True,            # Different noise per pixel\n",
    "        p=1.0\n",
    "    ),\n",
    "    A.RandomBrightnessContrast(      # Adjust brightness and contrast\n",
    "        brightness_limit=0.2,        # Â±20% brightness change\n",
    "        contrast_limit=0.2,          # Â±20% contrast change\n",
    "        p=1.0\n",
    "    )\n",
    "], n=1, p=0.6)                       # Choose 1 of them with 60% probability overall\n",
    "\n",
    "# --- Training transform pipeline ---\n",
    "transform = A.Compose([\n",
    "    # Crop and resize while maintaining aspect ratio\n",
    "    A.RandomResizedCrop(\n",
    "        height=config.img_size,\n",
    "        width=config.img_size,\n",
    "        scale=(0.8, 1.0),\n",
    "        p=1.0\n",
    "    ),\n",
    "\n",
    "    geo_aug,  # Apply geometric augmentations\n",
    "\n",
    "    # Randomly mask out rectangular regions (like Cutout)\n",
    "    A.CoarseDropout(\n",
    "        max_holes=2,\n",
    "        max_height=60, max_width=60,\n",
    "        min_height=20, min_width=20,\n",
    "        fill_value=0,\n",
    "        p=0.4\n",
    "    ),\n",
    "\n",
    "    photo_aug,  # Apply photometric augmentations\n",
    "\n",
    "    # Randomly apply blur\n",
    "    A.OneOf([\n",
    "        A.GaussianBlur(blur_limit=7, sigma_limit=(0.1, 2.0), p=1.0),\n",
    "        A.MotionBlur(blur_limit=(3, 5), p=1.0)\n",
    "    ], p=0.2),\n",
    "\n",
    "    # Normalize to mean=0.5, std=0.5 (grayscale)\n",
    "    A.Normalize(mean=[0.5], std=[0.5]),\n",
    "\n",
    "    # Convert to PyTorch tensor (also transpose mask dims)\n",
    "    ToTensorV2(transpose_mask=True)\n",
    "],\n",
    "    additional_targets={'scribble': 'mask'}  # Treat 'scribble' key as a mask for augmentation\n",
    ")\n",
    "\n",
    "# --- Test transform pipeline (no heavy augmentation) ---\n",
    "transform_test = A.Compose([\n",
    "    A.Resize(height=config.img_size, width=config.img_size),\n",
    "    A.Normalize(mean=[0.5], std=[0.5]),\n",
    "    ToTensorV2(transpose_mask=True)\n",
    "],\n",
    "    additional_targets={'scribble': 'mask'}\n",
    ")\n",
    "\n",
    "# --- Validation transform pipeline (same as test) ---\n",
    "transform_val = A.Compose([\n",
    "    A.Resize(height=config.img_size, width=config.img_size),\n",
    "    A.Normalize(mean=[0.5], std=[0.5]),\n",
    "    ToTensorV2(transpose_mask=True)\n",
    "],\n",
    "    additional_targets={'scribble': 'mask'}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d4abaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# Load dataset splits\n",
    "# ======================\n",
    "\n",
    "# --- Training set ---\n",
    "# Collect all complete volumes (imageâ€“mask pairs with all slices present)\n",
    "train_imgs, train_msks = collect_complete_volumes(\n",
    "    data_dir,\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "# --- Validation set ---\n",
    "val_imgs, val_msks = collect_complete_volumes(\n",
    "    data_dir,\n",
    "    split=\"val\"\n",
    ")\n",
    "\n",
    "# --- Test set ---\n",
    "# Here test set is kept separate (not merged with validation)\n",
    "test_imgs, test_msks = collect_complete_volumes(\n",
    "    data_dir,\n",
    "    split=\"test\"\n",
    ")\n",
    "\n",
    "# ======================\n",
    "# Dataset summary\n",
    "# ======================\n",
    "# Verify the number of complete volumes and slice counts for each split\n",
    "summary(\"train\", train_imgs, train_msks)\n",
    "summary(\"val\",   val_imgs,   val_msks)\n",
    "summary(\"test\",  test_imgs,  test_msks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0586aeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# Map patient IDs â†’ list of (time index, dataset index)\n",
    "# ======================\n",
    "patient2indices = defaultdict(list)\n",
    "\n",
    "# Loop through all training volumes\n",
    "for ds_idx, imgs in enumerate(train_imgs):\n",
    "    pid, t_idx = get_pid_time(imgs[0])  # Extract patient ID and time index from first slice filename\n",
    "    patient2indices[pid].append((t_idx, ds_idx))  # Append tuple (time index, dataset index)\n",
    "\n",
    "# Sort each patient's list of volumes by time index (chronological order)\n",
    "for pid in patient2indices:\n",
    "    patient2indices[pid].sort(key=lambda x: x[0])\n",
    "\n",
    "# ======================\n",
    "# Custom batch sampler for temporal windows\n",
    "# ======================\n",
    "sampler = NeighborTimeBatchSampler(\n",
    "    patient2indices,\n",
    "    window_size=config.window_size,      # e.g. 5 phases per batch\n",
    "    stride=config.stride,                # e.g. overlap by 3 phases: [t0â€“t4], [t2â€“t6], â€¦\n",
    "    shuffle_patients=True,               # Shuffle patient order each epoch\n",
    "    drop_last=False                      # Keep partial windows at the end\n",
    ")\n",
    "\n",
    "# ======================\n",
    "# DataLoader without shuffling (temporal windows preserved)\n",
    "# ======================\n",
    "train_loader_noMIX = DataLoader(\n",
    "    ReadChunkDatasetScribble(\n",
    "        train_imgs,\n",
    "        train_msks,\n",
    "        transform=transform\n",
    "    ),\n",
    "    batch_sampler=sampler,                # Uses temporal neighbor sampler\n",
    "    num_workers=config.num_workers,       # Parallel data loading\n",
    "    pin_memory=True                       # Faster host-to-GPU transfer\n",
    ")\n",
    "\n",
    "# ======================\n",
    "# DataLoader with shuffling (random mixing of samples)\n",
    "# ======================\n",
    "train_loader_MIX = DataLoader(\n",
    "    ReadChunkDatasetScribble(\n",
    "        train_imgs,\n",
    "        train_msks,\n",
    "        transform=transform\n",
    "    ),\n",
    "    batch_size=config.trainBatch,         # Fixed batch size\n",
    "    shuffle=True,                          # Randomize order\n",
    "    num_workers=config.num_workers\n",
    ")\n",
    "\n",
    "# ======================\n",
    "# Inspect the first batch from the temporal-window loader\n",
    "# ======================\n",
    "for i, batch in enumerate(train_loader_noMIX):\n",
    "    print(f'-------Batch={i}-----------')\n",
    "    pid_set = {get_pid_time(p)[0] for p in batch['name'][0]}  # Unique patient IDs in batch\n",
    "    times   = [get_pid_time(p)[1] for p in batch['name'][0]]  # Corresponding time indices\n",
    "    print(\"patient(s) in first batch:\", pid_set)\n",
    "    print(\"time indices:\", times[:config.window_size])\n",
    "    print(\"image tensor shape:\", batch['image'].shape)        # Shape: [B, 37, 1, H, W]\n",
    "    print()\n",
    "    print(batch['name'][0][0])                                # Path of first slice in batch\n",
    "    break\n",
    "\n",
    "# ======================\n",
    "# Inspect the first batch from the random-mix loader\n",
    "# ======================\n",
    "for i, batch in enumerate(train_loader_MIX):\n",
    "    print(f'-------Batch={i}-----------')\n",
    "    pid_set = {get_pid_time(p)[0] for p in batch['name'][0]}\n",
    "    times   = [get_pid_time(p)[1] for p in batch['name'][0]]\n",
    "    print(\"patient(s) in first batch:\", pid_set)\n",
    "    print(\"time indices:\", times[:config.window_size])\n",
    "    print(\"image tensor shape:\", batch['image'].shape)        # Shape: [B, 37, 1, H, W]\n",
    "    print()\n",
    "    print(batch['name'][0][0])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc40265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# Test DataLoader (no shuffling, no strong augmentations)\n",
    "# ======================\n",
    "test_loader_MIX = DataLoader(\n",
    "    ReadChunkDatasetScribble(\n",
    "        test_imgs,\n",
    "        test_msks,\n",
    "        transform=transform_test    # Test-time transform (resize + normalize only)\n",
    "    ),\n",
    "    batch_size=config.testBatch,    # Number of volumes per batch during testing\n",
    "    shuffle=False,                  # Keep test set order fixed (no shuffling)\n",
    "    num_workers=config.num_workers  # Number of parallel data-loading workers\n",
    ")\n",
    "\n",
    "# ======================\n",
    "# Inspect the first test batch\n",
    "# ======================\n",
    "for i, batch in enumerate(test_loader_MIX):\n",
    "    print(f'-------Batch={i}-----------')\n",
    "\n",
    "    # Extract unique patient IDs in this batch (from first slice in each volume)\n",
    "    pid_set = {get_pid_time(p)[0] for p in batch['name'][0]}\n",
    "\n",
    "    # Extract corresponding time indices for each volume\n",
    "    times = [get_pid_time(p)[1] for p in batch['name'][0]]\n",
    "\n",
    "    print(\"patient(s) in first batch:\", pid_set)\n",
    "    print(\"time indices:\", times[:config.window_size])        # Show up to window_size frames\n",
    "\n",
    "    # Show tensor shape: [B, 37 slices, 1 channel, H, W]\n",
    "    print(\"image tensor shape:\", batch['image'].shape)\n",
    "\n",
    "    # Print path of the first slice in the first volume of this batch\n",
    "    print()\n",
    "    print(batch['name'][0][0])\n",
    "\n",
    "    break  # Only inspect the first batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ff58a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# Validation DataLoader (with shuffling)\n",
    "# ======================\n",
    "val_loader_MIX = DataLoader(\n",
    "    ReadChunkDatasetScribble(\n",
    "        val_imgs,\n",
    "        val_msks,\n",
    "        transform=transform_val      # Validation transform (resize + normalize)\n",
    "    ),\n",
    "    batch_size=config.valBatch,      # Number of volumes per validation batch\n",
    "    shuffle=True,                    # Shuffle order for validation batches\n",
    "    num_workers=config.num_workers   # Parallel data loading workers\n",
    ")\n",
    "\n",
    "# ======================\n",
    "# Inspect the first validation batch\n",
    "# ======================\n",
    "for i, batch in enumerate(val_loader_MIX):\n",
    "    print(f'-------Batch={i}-----------')\n",
    "\n",
    "    # Extract unique patient IDs from the batch\n",
    "    pid_set = {get_pid_time(p)[0] for p in batch['name'][0]}\n",
    "\n",
    "    # Extract corresponding time indices for each volume\n",
    "    times = [get_pid_time(p)[1] for p in batch['name'][0]]\n",
    "\n",
    "    print(\"patient(s) in first batch:\", pid_set)\n",
    "    print(\"time indices:\", times[:config.window_size])         # Display only first window_size entries\n",
    "\n",
    "    # Show tensor shape: [B, 37 slices, 1 channel, H, W]\n",
    "    print(\"image tensor shape:\", batch['image'].shape)\n",
    "\n",
    "    # Print path to the first slice in the first volume\n",
    "    print()\n",
    "    print(batch['name'][0][0])\n",
    "\n",
    "    break  # Stop after inspecting first batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d131030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function: return the first batch from a DataLoader\n",
    "def first(loader):\n",
    "    return next(iter(loader))\n",
    "\n",
    "# ======================\n",
    "# Inspect a sample training batch\n",
    "# ======================\n",
    "train_sample = first(train_loader_MIX)\n",
    "print(\"Train batch shapes:\")\n",
    "print(\"  image:    \", train_sample['image'].shape)     # Shape: [B, 37 slices, 1 channel, H, W]\n",
    "print(\"  mask:     \", train_sample['mask'].shape)      # Same shape as image\n",
    "print(\"  scribble: \", train_sample['scribble'].shape)  # Same shape as mask, scribble annotations\n",
    "print(\"  names in volume 0:\", train_sample['name'][0][:3], \"...\")  # First 3 slice filenames\n",
    "\n",
    "# ======================\n",
    "# Inspect a sample test batch\n",
    "# ======================\n",
    "test_sample = first(test_loader_MIX)\n",
    "print(\"\\nTest batch shapes:\")\n",
    "print(\"  image:    \", test_sample['image'].shape)\n",
    "print(\"  mask:     \", test_sample['mask'].shape)\n",
    "print(\"  scribble: \", test_sample['scribble'].shape)\n",
    "print(\"  names in volume 0:\", test_sample['name'][0][:3], \"...\")\n",
    "\n",
    "# ======================\n",
    "# Inspect a sample validation batch\n",
    "# ======================\n",
    "val_sample = first(val_loader_MIX)\n",
    "print(\"\\nVal batch shapes:\")\n",
    "print(\"  image:    \", val_sample['image'].shape)\n",
    "print(\"  mask:     \", val_sample['mask'].shape)\n",
    "print(\"  scribble: \", val_sample['scribble'].shape)\n",
    "print(\"  names in volume 0:\", val_sample['name'][0][:3], \"...\")\n",
    "\n",
    "# ======================\n",
    "# One-hot encode the masks from the test batch\n",
    "# ======================\n",
    "# Convert mask tensor [B, 37, 1, H, W] â†’ one-hot encoding [B, 37, C, H, W]\n",
    "#   - C=3: background + 2 foreground classes\n",
    "one_hot_mask = make_one_hot(test_sample['mask'], device, C=3)\n",
    "print(one_hot_mask.shape)  # Example: [4, 37, 3, 256, 256]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115f7e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Example: visualize scribble annotations for training data\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Get the first batch from the training loader\n",
    "batch = next(iter(train_loader_MIX))   # Could also use: batch = first(train_loader_MIX)\n",
    "\n",
    "# Print the filename of the first slice in the first volume\n",
    "print(batch['name'][0][0])\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Visualization #1: Single slice across all time frames\n",
    "#   - slice_idx=0 â†’ show the same spatial slice (0) for every time frame in the batch\n",
    "#   - num_classes=config.num_classes â†’ total segmentation classes\n",
    "#   - device=device â†’ where to run one-hot encoding\n",
    "# -------------------------------------------------\n",
    "show_time_slices_scribble(\n",
    "    batch=batch,\n",
    "    slice_idx=0,\n",
    "    num_classes=config.num_classes,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Visualization #2: Multiple slices from a single time frame\n",
    "#   - volume_idx=0 â†’ fix to first time frame (T0) in the batch\n",
    "#   - slice_indices=[0, 6, 12, 18, 24, 30] â†’ select subset of spatial slices (0â€“36)\n",
    "# -------------------------------------------------\n",
    "show_slices_scribble(\n",
    "    batch=batch,\n",
    "    volume_idx=0,                       # Choose a single time frame to display\n",
    "    slice_indices=[0, 6, 12, 18, 24, 30], # Pick any subset of slice indices\n",
    "    num_classes=config.num_classes,\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bb35b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Example: visualize scribble annotations for temporally ordered batches\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Get the first batch from the temporal-window training loader (no mixing of time frames)\n",
    "batch = next(iter(train_loader_noMIX))   # Could also use: batch = first(train_loader_noMIX)\n",
    "\n",
    "# Print the filename of the first slice in the first volume\n",
    "print(batch['name'][0][0])\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Visualization #1: Single spatial slice across all time frames\n",
    "#   - slice_idx=0 â†’ display the same radial slice index (0) for every time frame\n",
    "#   - num_classes=config.num_classes â†’ number of segmentation classes\n",
    "#   - device=device â†’ used for one-hot encoding inside the function\n",
    "# -------------------------------------------------\n",
    "show_time_slices_scribble(\n",
    "    batch=batch,\n",
    "    slice_idx=0,\n",
    "    num_classes=config.num_classes,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Visualization #2: Multiple spatial slices from a single time frame\n",
    "#   - volume_idx=0 â†’ choose the first time frame in the batch\n",
    "#   - slice_indices=[0, 6, 12, 18, 24, 30] â†’ select subset of the 37 available slices\n",
    "# -------------------------------------------------\n",
    "show_slices_scribble(\n",
    "    batch=batch,\n",
    "    volume_idx=0,                        # Fix to one time frame (e.g., T0)\n",
    "    slice_indices=[0, 6, 12, 18, 24, 30], # Pick any subset of slice IDs (0â€“36)\n",
    "    num_classes=config.num_classes,\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db4048e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Example: visualize scribble annotations for test data\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Get the first batch from the test loader\n",
    "batch = next(iter(test_loader_MIX))   # Could also use: batch = first(test_loader_MIX)\n",
    "\n",
    "# Print the filename of the first slice in the first volume of the batch\n",
    "print(batch['name'][0][0])\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Visualization #1: Single spatial slice across all time frames\n",
    "#   - slice_idx=0 â†’ show the same radial slice index (0) for each time frame\n",
    "#   - num_classes=config.num_classes â†’ total number of segmentation classes\n",
    "#   - device=device â†’ used for one-hot encoding inside the function\n",
    "# -------------------------------------------------\n",
    "show_time_slices_scribble(\n",
    "    batch=batch,\n",
    "    slice_idx=0,\n",
    "    num_classes=config.num_classes,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Visualization #2: Multiple spatial slices from a single time frame\n",
    "#   - volume_idx=0 â†’ fix to the first time frame (T0) in the batch\n",
    "#   - slice_indices=[0, 6, 12, 18, 24, 30] â†’ choose any subset of slices (0â€“36)\n",
    "# -------------------------------------------------\n",
    "show_slices_scribble(\n",
    "    batch=batch,\n",
    "    volume_idx=0,                        # Pick one time frame\n",
    "    slice_indices=[0, 6, 12, 18, 24, 30], # Select a subset of slice IDs\n",
    "    num_classes=config.num_classes,\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36e18e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Example: visualize scribble annotations for validation data\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Get the first batch from the validation loader\n",
    "batch = next(iter(val_loader_MIX))   # Could also use: batch = first(val_loader_MIX)\n",
    "\n",
    "# Print the filename of the first slice in the first volume of the batch\n",
    "print(batch['name'][0][0])\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Visualization #1: Single spatial slice across all time frames\n",
    "#   - slice_idx=0 â†’ show the same radial slice index (0) for each time frame\n",
    "#   - num_classes=config.num_classes â†’ number of segmentation classes\n",
    "#   - device=device â†’ used for one-hot encoding inside the visualization function\n",
    "# -------------------------------------------------\n",
    "show_time_slices_scribble(\n",
    "    batch=batch,\n",
    "    slice_idx=0,\n",
    "    num_classes=config.num_classes,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Visualization #2: Multiple spatial slices from a single time frame\n",
    "#   - volume_idx=0 â†’ fix to the first time frame (T0) in the batch\n",
    "#   - slice_indices=[0, 6, 12, 18, 24, 30] â†’ subset of available slices (0â€“36)\n",
    "# -------------------------------------------------\n",
    "show_slices_scribble(\n",
    "    batch=batch,\n",
    "    volume_idx=0,                        # Pick one time frame to display\n",
    "    slice_indices=[0, 6, 12, 18, 24, 30], # Select subset of slice IDs\n",
    "    num_classes=config.num_classes,\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2635586c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbeddingRandom(nn.Module):\n",
    "    \"\"\"\n",
    "    SAM-style random Fourier positional encoding.\n",
    "\n",
    "    This creates a fixed, random Gaussian projection matrix and uses\n",
    "    sine/cosine embeddings (similar to random Fourier features) to \n",
    "    encode (x, y) pixel coordinates.\n",
    "\n",
    "    Args:\n",
    "        num_pos_feats (int): Number of features per coordinate axis.\n",
    "                             Output channels will be 2 * num_pos_feats (sin + cos).\n",
    "        scale (float): Scaling factor applied to the random projection matrix.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_pos_feats: int = 64, scale: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\n",
    "            \"positional_encoding_gaussian_matrix\",\n",
    "            scale * torch.randn((2, num_pos_feats))  # [xy, num_pos_feats]\n",
    "        )\n",
    "\n",
    "    def _pe_encoding(self, coords: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode normalized coords âˆˆ [-1, 1] with sin/cos random features.\n",
    "        Args:\n",
    "            coords: Tensor of shape [N, 2] (x, y) normalized coordinates.\n",
    "        Returns:\n",
    "            Tensor of shape [N, 2*num_pos_feats] (sin and cos concatenated).\n",
    "        \"\"\"\n",
    "        coords = 2 * coords - 1                                  # map [0,1] â†’ [-1,1]\n",
    "        coords = coords @ self.positional_encoding_gaussian_matrix  # linear projection\n",
    "        coords = 2 * torch.pi * coords\n",
    "        return torch.cat([torch.sin(coords), torch.cos(coords)], dim=-1)\n",
    "\n",
    "    def forward(self, size: Tuple[int, int]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Create a full [C, H, W] positional encoding map.\n",
    "        Args:\n",
    "            size: (H, W) output resolution.\n",
    "        Returns:\n",
    "            Positional encoding tensor [C, H, W].\n",
    "        \"\"\"\n",
    "        h, w = size\n",
    "        device = self.positional_encoding_gaussian_matrix.device\n",
    "\n",
    "        # Create normalized coordinate grids\n",
    "        grid_y = torch.arange(h, device=device).float() + 0.5\n",
    "        grid_x = torch.arange(w, device=device).float() + 0.5\n",
    "        y_embed = grid_y[:, None].repeat(1, w) / h\n",
    "        x_embed = grid_x[None, :].repeat(h, 1) / w\n",
    "        coords = torch.stack([x_embed, y_embed], dim=-1)  # [H, W, 2]\n",
    "\n",
    "        # Encode and reshape to [C, H, W]\n",
    "        return self._pe_encoding(coords.view(-1, 2)).view(h, w, -1).permute(2, 0, 1)\n",
    "\n",
    "\n",
    "class ScribblePositionEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Converts binary scribble masks into dense prompt embeddings \n",
    "    for integration into vision models.\n",
    "\n",
    "    Workflow:\n",
    "      1. Downsample scribble mask to match feature map resolution.\n",
    "      2. Add random Fourier positional encoding (PE).\n",
    "      3. Add a learned \"scribble token\" embedding at scribble locations.\n",
    "      4. Smooth with small CNN.\n",
    "\n",
    "    Args:\n",
    "        embed_dim (int): Output embedding channel dimension.\n",
    "        image_size (H_img, W_img): Original input image size.\n",
    "        feature_size (H_feat, W_feat): Target feature map size to align with backbone.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int = 256,\n",
    "        image_size: Tuple[int, int] = (1024, 1024),\n",
    "        feature_size: Tuple[int, int] = (64, 64)\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.image_size = image_size\n",
    "        self.feature_size = feature_size\n",
    "\n",
    "        # Positional encoding (random Fourier features)\n",
    "        self.pe_layer = PositionEmbeddingRandom(embed_dim // 2)\n",
    "\n",
    "        # Learned scribble token (single class: scribble foreground)\n",
    "        self.scribble_embed = nn.Embedding(1, embed_dim)\n",
    "\n",
    "        # Small CNN to smooth/aggregate encoded scribble features\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(embed_dim, embed_dim, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(embed_dim, embed_dim, 3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, scribbles: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            scribbles: [B, 1, H_img, W_img] binary mask (values {0,1} or {0,255}).\n",
    "        Returns:\n",
    "            Tensor [B, embed_dim, H_feat, W_feat]: dense prompt encoding.\n",
    "        \"\"\"\n",
    "        B, _, H_img, W_img = scribbles.shape\n",
    "        H_feat, W_feat = self.feature_size\n",
    "        device = scribbles.device\n",
    "\n",
    "        # Sanity check: enforce valid binary mask values\n",
    "        unique_vals = torch.unique(scribbles)\n",
    "        if not torch.all((unique_vals == 0) | (unique_vals == 1) | (unique_vals == 255)):\n",
    "            raise ValueError(\n",
    "                f\"Scribble mask must contain only 0, 1, or 255 â€” found {unique_vals.tolist()}\"\n",
    "            )\n",
    "\n",
    "        # Normalize mask to float [0,1]\n",
    "        if scribbles.max() > 1:\n",
    "            scribbles = (scribbles > 0).float()  # Convert 0/255 to 0.0/1.0\n",
    "        else:\n",
    "            scribbles = scribbles.float()\n",
    "\n",
    "        # Downsample to match feature resolution\n",
    "        scrib_down = F.interpolate(scribbles, size=(H_feat, W_feat), mode=\"nearest\")\n",
    "\n",
    "        # Positional encoding map [C, H_feat, W_feat]\n",
    "        pe = self.pe_layer(self.feature_size).to(device)\n",
    "        pe = pe.unsqueeze(0).expand(B, -1, -1, -1)  # Broadcast to batch\n",
    "\n",
    "        # Learned scribble token embedding [1, C, 1, 1]\n",
    "        token = self.scribble_embed.weight.view(1, -1, 1, 1)\n",
    "\n",
    "        # Combine PE and token at scribble locations\n",
    "        encoded = pe * scrib_down + token * scrib_down\n",
    "\n",
    "        # Smooth/aggregate features\n",
    "        return self.conv(encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f8a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard double-convolution block:\n",
    "        Conv â†’ BatchNorm â†’ ReLU â†’ Conv â†’ BatchNorm â†’ ReLU\n",
    "    Preserves spatial resolution (padding=1 for 3Ã—3 kernels).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(input_channels, output_channels, kernel_size=3, padding=1)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(output_channels)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(output_channels, output_channels, kernel_size=3, padding=1)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(output_channels)\n",
    "        \n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Two-convolution residual block with optional channel-matching skip path.\n",
    "    Structure:\n",
    "        Conv â†’ BN â†’ ReLU â†’ Conv â†’ BN â†’ (Add skip) â†’ ReLU\n",
    "\n",
    "    If input and output channels differ, a 1Ã—1 Conv+BN is applied\n",
    "    on the skip connection to match dimensions.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels: int, output_channels: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # Main path\n",
    "        self.conv1 = nn.Conv2d(input_channels, output_channels,\n",
    "                               kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(output_channels)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(output_channels, output_channels,\n",
    "                               kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(output_channels)\n",
    "\n",
    "        # Skip path: identity if same channels, else 1Ã—1 Conv+BN\n",
    "        self.skip = (\n",
    "            nn.Identity() if input_channels == output_channels else\n",
    "            nn.Conv2d(input_channels, output_channels, kernel_size=1, bias=False)\n",
    "        )\n",
    "        self.skip_bn = (\n",
    "            nn.Identity() if input_channels == output_channels else\n",
    "            nn.BatchNorm2d(output_channels)\n",
    "        )\n",
    "\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Skip connection (possibly channel-adjusted)\n",
    "        identity = self.skip_bn(self.skip(x))\n",
    "\n",
    "        # Main path\n",
    "        out = self.act(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "\n",
    "        # Residual addition\n",
    "        out += identity\n",
    "        out = self.act(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder stage:\n",
    "      - Residual block for feature extraction\n",
    "      - Max pooling to downsample by factor 2\n",
    "\n",
    "    Returns:\n",
    "        feature_map        : output of conv block (for skip connection)\n",
    "        pooled_feature_map : downsampled features (to next encoder stage)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv_block = ResidualBlock(input_channels, output_channels)\n",
    "        self.pooling = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature_map = self.conv_block(x)\n",
    "        pooled_feature_map = self.pooling(feature_map)\n",
    "        return feature_map, pooled_feature_map\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder stage:\n",
    "      - Transposed convolution to upsample by factor 2\n",
    "      - Concatenate with corresponding encoder skip connection\n",
    "      - Residual block for feature refinement\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.upsample = nn.ConvTranspose2d(\n",
    "            input_channels, output_channels, kernel_size=2, stride=2\n",
    "        )\n",
    "        # After concatenation: output_channels (upsampled) + output_channels (skip)\n",
    "        self.conv_block = ResidualBlock(output_channels * 2, output_channels)\n",
    "\n",
    "    def forward(self, x, skip_connection):\n",
    "        # Upsample decoder features\n",
    "        x = self.upsample(x)\n",
    "\n",
    "        # Concatenate with encoder features (skip connection)\n",
    "        x = torch.cat([x, skip_connection], dim=1)\n",
    "\n",
    "        # Apply convolutional refinement\n",
    "        x = self.conv_block(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc578490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlipConsistentAttention import *\n",
    "\n",
    "class SCOPENet(nn.Module):\n",
    "    def __init__(self, num_input_channels, num_classes, feature_sizes=[64, 128, 256, 512, 1024]):\n",
    "        super().__init__()\n",
    "\n",
    "        # -------------------------\n",
    "        # Encoder path (downsampling)\n",
    "        # -------------------------\n",
    "        self.encoder1 = EncoderBlock(num_input_channels, feature_sizes[0])  # C â†’ 64\n",
    "        self.encoder2 = EncoderBlock(feature_sizes[0], feature_sizes[1])    # 64 â†’ 128\n",
    "        self.encoder3 = EncoderBlock(feature_sizes[1], feature_sizes[2])    # 128 â†’ 256\n",
    "        self.encoder4 = EncoderBlock(feature_sizes[2], feature_sizes[3])    # 256 â†’ 512\n",
    "        self.bottleneck = ResidualBlock(feature_sizes[3], feature_sizes[4]) # 512 â†’ 1024 (no pooling)\n",
    "\n",
    "        # Flip-consistent attention over radial slices (operates on [B,S,C,H,W])\n",
    "        self.attentions = FCRA(dim=feature_sizes[4])\n",
    "\n",
    "        # -------------------------\n",
    "        # Scribble prompt encoder\n",
    "        # -------------------------\n",
    "        self.scribble_encoder = ScribblePositionEncoder(\n",
    "            embed_dim=feature_sizes[4],         # 1024 channels to match bottleneck\n",
    "            image_size=(256, 256),              # original image size (for reference)\n",
    "            feature_size=(16, 16)               # target feature map size for prompts\n",
    "        )\n",
    "\n",
    "        # -------------------------\n",
    "        # Decoder path (upsampling)\n",
    "        # -------------------------\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            DecoderBlock(feature_sizes[i + 1], feature_sizes[i])  # 1024â†’512, 512â†’256, 256â†’128, 128â†’64\n",
    "            for i in range(len(feature_sizes) - 1)\n",
    "        ])\n",
    "\n",
    "        # Final 1Ã—1 conv to produce class logits; Softmax for probabilities\n",
    "        self.classifier = nn.Conv2d(feature_sizes[0], num_classes, kernel_size=1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "\n",
    "    def forward(self, x, scribble):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x:        [B, S, C, H, W]  volume of slices per sample\n",
    "            scribble: [B, S, C, H, W]  scribble masks aligned with x\n",
    "                                       (NOTE: scribble encoder expects a single-channel mask per slice;\n",
    "                                        here 'C' should effectively be 1 for scribbles.)\n",
    "        Returns:\n",
    "            segmentation_mask: [B, S, num_classes, H, W]\n",
    "            bottleneck_feats : [B, S, C_bottleneck, H', W'] features before decoder\n",
    "        \"\"\"\n",
    "        B, S, C, H, W = x.shape\n",
    "\n",
    "        # Flatten temporal/slice dimension into batch for 2D encoders\n",
    "        x = x.view(B * S, C, H, W)                     # [B*S, C, H, W]\n",
    "        scribble = scribble.view(B * S, C, H, W)       # [B*S, C, H, W]\n",
    "\n",
    "        # -------------------------\n",
    "        # Encoder forward (with skips)\n",
    "        # -------------------------\n",
    "        skip1, p1 = self.encoder1(x)                   # skip1: [B*S, 64,  H,   W  ], p1: [B*S, 64,  H/2,  W/2]\n",
    "        skip2, p2 = self.encoder2(p1)                  # skip2: [B*S, 128, H/2, W/2], p2: [B*S, 128, H/4,  W/4]\n",
    "        skip3, p3 = self.encoder3(p2)                  # skip3: [B*S, 256, H/4, W/4], p3: [B*S, 256, H/8,  W/8]\n",
    "        skip4, p4 = self.encoder4(p3)                  # skip4: [B*S, 512, H/8, W/8], p4: [B*S, 512, H/16, W/16]\n",
    "        bottleneck = self.bottleneck(p4)               # [B*S, 1024, H/16, W/16]\n",
    "\n",
    "        # Flip-consistent attention across S (reshape to [B,S,C,H',W'])\n",
    "        bottleneck = self.attentions(\n",
    "            bottleneck.reshape(B, S, *bottleneck.shape[1:])\n",
    "        ).reshape(B * S, *bottleneck.shape[1:])        # back to [B*S, 1024, H/16, W/16]\n",
    "\n",
    "        # Encode scribble prompt at feature resolution\n",
    "        scribble_prompt = self.scribble_encoder(scribble)  # [B*S, 1024, H/16, W/16]\n",
    "\n",
    "        # Save bottleneck feats per [B,S,...] for possible analysis/outputs\n",
    "        bottleneck_feats = bottleneck.view(B, S, *bottleneck.shape[1:])  # [B, S, 1024, H/16, W/16]\n",
    "\n",
    "        # Modulate bottleneck with scribble prompt (elementwise)\n",
    "        bottleneck = bottleneck * scribble_prompt         # [B*S, 1024, H/16, W/16]\n",
    "\n",
    "        # -------------------------\n",
    "        # Decoder forward (skip connections)\n",
    "        # Notes:\n",
    "        #   skip4, skip3, skip2, skip1 correspond to encoder stages 4â†’1\n",
    "        #   decoded progressively returns to input spatial size\n",
    "        # -------------------------\n",
    "        decoded = self.decoder_blocks[3](bottleneck, skip4)  # 1024â†’512, concat with skip4 â†’ ResidualBlock\n",
    "        decoded = self.decoder_blocks[2](decoded,   skip3)   # 512â†’256\n",
    "        decoded = self.decoder_blocks[1](decoded,   skip2)   # 256â†’128\n",
    "        decoded = self.decoder_blocks[0](decoded,   skip1)   # 128â†’64\n",
    "\n",
    "        # -------------------------\n",
    "        # Segmentation head\n",
    "        # -------------------------\n",
    "        segmentation_mask = self.classifier(decoded)          # [B*S, num_classes, H, W]\n",
    "        segmentation_mask = self.softmax(segmentation_mask)   # probabilities per class (optional for training)\n",
    "\n",
    "        # Restore [B, S, num_classes, H, W]\n",
    "        segmentation_mask = segmentation_mask.view(B, S, self.classifier.out_channels, H, W)\n",
    "\n",
    "        return segmentation_mask, bottleneck_feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ebf6c0",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Instantiate the SCOPENet segmentation model\n",
    "#   - num_classes: number of segmentation classes (including background)\n",
    "#   - num_input_channels: number of input channels per slice (e.g., 1 for grayscale)\n",
    "model = SCOPENet(\n",
    "    num_classes=config.num_classes,\n",
    "    num_input_channels=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b093f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceIoU(nn.Module):\n",
    "    \"\"\" Computes Dice coefficient and Intersection over Union (IoU) for each class separately. \"\"\"\n",
    "    def __init__(self, num_classes, smooth=1e-5):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes         # number of semantic classes (incl. background)\n",
    "        self.smooth = smooth                   # epsilon to avoid zero-division\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        \"\"\" Computes Dice coefficient and IoU for each class. \n",
    "        \n",
    "        Args:\n",
    "        - predictions: Tensor of shape (B, S, C, H, W) or (B, C, H, W) -> per-class probabilities.\n",
    "        - targets    : Tensor of shape (B, S, C, H, W) or (B, C, H, W) -> one-hot ground truth.\n",
    "\n",
    "        Returns:\n",
    "        - dice_scores: List of Dice scores (scalar tensors), length = num_classes.\n",
    "        - iou_scores : List of IoU scores  (scalar tensors), length = num_classes.\n",
    "        \"\"\"\n",
    "        dice_scores = []                       # will collect per-class Dice\n",
    "        iou_scores  = []                       # will collect per-class IoU\n",
    "        \n",
    "        for class_idx in range(self.num_classes):\n",
    "            # Select the class channel across batch / (optional) time / spatial dims\n",
    "            class_prediction = predictions[:, :, class_idx, :, :]  # prob map for class k\n",
    "            class_target     = targets[:,     :, class_idx, :, :]  # one-hot GT for class k\n",
    "            \n",
    "            # Intersection and (soft) union across all dims\n",
    "            intersection = torch.sum(class_prediction * class_target)     # overlap\n",
    "            total_sum    = torch.sum(class_prediction) + torch.sum(class_target)\n",
    "            union        = total_sum - intersection                       # |A âˆª B| = |A|+|B|-|Aâˆ©B|\n",
    "            \n",
    "            # Dice = (2|Aâˆ©B|) / (|A|+|B|)\n",
    "            dice_score = (2. * intersection + self.smooth) / (total_sum + self.smooth)\n",
    "            dice_scores.append(dice_score)\n",
    "\n",
    "            # IoU = |Aâˆ©B| / |AâˆªB|\n",
    "            iou_score = (intersection + self.smooth) / (union + self.smooth)\n",
    "            iou_scores.append(iou_score)\n",
    "        \n",
    "        return dice_scores, iou_scores\n",
    "\n",
    "\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    \"\"\" Computes the Dice loss for multi-class segmentation (averaged over classes, excluding background). \"\"\"\n",
    "    def __init__(self, num_classes, smooth=1e-5):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes         # total classes (assume class 0 is background)\n",
    "        self.smooth = smooth                   # epsilon for numerical stability\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        \"\"\" Computes the Dice loss, averaging across all classes (excluding background). \"\"\"\n",
    "        dice_losses = []                       # collect 1 - Dice for each foreground class\n",
    "        for class_idx in range(1, self.num_classes):  # Skip background (index 0)\n",
    "            class_prediction = predictions[:, :, class_idx, :, :]  # predicted prob map for class\n",
    "            class_target     = targets[:,     :, class_idx, :, :]  # one-hot GT for class\n",
    "\n",
    "            # Dice components\n",
    "            intersection = torch.sum(class_prediction * class_target)\n",
    "            union        = torch.sum(class_prediction) + torch.sum(class_target)\n",
    "            dice_score   = (2. * intersection + self.smooth) / (union + self.smooth)\n",
    "            \n",
    "            dice_loss = 1. - dice_score        # Dice loss\n",
    "            dice_losses.append(dice_loss)\n",
    "        \n",
    "        return torch.mean(torch.stack(dice_losses))  # mean over foreground classes\n",
    "\n",
    "\n",
    "# Define loss functions (module instances)\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()                 # per-pixel multi-class CE on logits\n",
    "dice_multiclass    = DiceIoU(num_classes=config.num_classes)\n",
    "dice_loss_fn       = DiceLoss(num_classes=config.num_classes)\n",
    "\n",
    "\n",
    "def symmetry_consistency_loss(f: torch.Tensor, mode: str = 'l1', margin: float = 0.9) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generic symmetry consistency loss.\n",
    "    Enforces: f[:, Î¸] â‰ˆ flip_W( f[:, Ï€âˆ’Î¸] )\n",
    "\n",
    "    Args:\n",
    "        f      : [B, S, C, H, W] tensor (e.g., bottleneck features or logits).\n",
    "        mode   : one of {'l1', 'cosine', 'margin'} controlling the penalty type.\n",
    "        margin : used only with 'margin' mode; target minimum cosine similarity.\n",
    "\n",
    "    Returns:\n",
    "        Scalar tensor: symmetry-consistency penalty.\n",
    "    \"\"\"\n",
    "    # Flip across angle Î¸ â†’ Ï€âˆ’Î¸  (reverse slice order)\n",
    "    f_flip = torch.flip(f, dims=[1])            # flip angular dimension S\n",
    "\n",
    "    # Then flip across width (leftâ€“right) to align mirrored content\n",
    "    f_flip = torch.flip(f_flip, dims=[-1])      # flip width W\n",
    "\n",
    "    if mode == 'l1':\n",
    "        return F.l1_loss(f, f_flip)             # mean absolute deviation\n",
    "\n",
    "    elif mode == 'cosine':\n",
    "        # Normalize flattened features per (B,S) before cosine similarity\n",
    "        f_flat      = F.normalize(f.view(f.shape[0], f.shape[1], -1),      dim=-1)  # [B, S, C*H*W]\n",
    "        f_flip_flat = F.normalize(f_flip.view(f.shape[0], f.shape[1], -1), dim=-1)\n",
    "        sim = F.cosine_similarity(f_flat, f_flip_flat, dim=-1)                       # [B, S]\n",
    "        return 1 - sim.mean()                   # penalize low similarity\n",
    "\n",
    "    elif mode == 'margin':\n",
    "        # Hinge-like margin on cosine similarity\n",
    "        f_flat      = F.normalize(f.view(f.shape[0], f.shape[1], -1),      dim=-1)\n",
    "        f_flip_flat = F.normalize(f_flip.view(f.shape[0], f.shape[1], -1), dim=-1)\n",
    "        sim = F.cosine_similarity(f_flat, f_flip_flat, dim=-1)                       # [B, S]\n",
    "        return torch.clamp(margin - sim, min=0).mean()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported mode: {mode}. Use 'l1', 'cosine', or 'margin'.\")\n",
    "\n",
    "\n",
    "def compute_loss(prediction, target, feats, device, Î»_sym: float = 0.1):\n",
    "    \"\"\"\n",
    "    Compute total training loss.\n",
    "\n",
    "    Components:\n",
    "      â€¢ Dice loss on predicted segmentation (via argmax â†’ one-hot).\n",
    "      â€¢ Cross-entropy loss against sparse labels.\n",
    "      â€¢ Symmetry-consistency loss on features/logits.\n",
    "\n",
    "    Args:\n",
    "        prediction : [B, S, C, H, W] model outputs (C=num_classes).\n",
    "                     CrossEntropyLoss expects **logits**; if Softmax was applied in the model,\n",
    "                     consider passing pre-Softmax logits instead for CE stability.\n",
    "        target     : [B, S, 1, H, W] integer class labels.\n",
    "        feats      : [B, S, C_f, H_f, W_f] feature map used for symmetry loss.\n",
    "        device     : torch.device for one-hot encoding.\n",
    "        Î»_sym      : weight for symmetry loss term.\n",
    "\n",
    "    Returns:\n",
    "        total_loss : scalar tensor\n",
    "        dice_scores: list of per-class Dice (as scalars)\n",
    "        IoU        : list of per-class IoU   (as scalars)\n",
    "        symm_loss  : scalar symmetry penalty\n",
    "    \"\"\"\n",
    "    # Convert network predictions to hard labels via argmax over class dim (dim=2 here)\n",
    "    segmented_predictions = torch.argmax(prediction, dim=2).unsqueeze(2).float()  # [B,S,1,H,W]\n",
    "\n",
    "    # Dice loss on one-hot encodings (predictions vs. targets)\n",
    "    dice_loss = dice_loss_fn(\n",
    "        make_one_hot(target, device, C=config.num_classes),\n",
    "        make_one_hot(segmented_predictions, device, C=config.num_classes)\n",
    "    ).requires_grad_(True)  # keep graph if needed\n",
    "\n",
    "    # Report per-class Dice / IoU (for logging/metrics)\n",
    "    dice_scores, IoU = dice_multiclass(\n",
    "        make_one_hot(target, device, C=config.num_classes),\n",
    "        make_one_hot(segmented_predictions, device, C=config.num_classes)\n",
    "    )\n",
    "\n",
    "    # Cross-entropy on sparse labels:\n",
    "    #   reshape to merge B and S so CE sees [N, C, H, W] vs. [N, H, W]\n",
    "    ce_logits = prediction.view(-1, prediction.size(2), prediction.size(3), prediction.size(4))  # [B*S, C, H, W]\n",
    "    ce_target = target.view(-1, target.size(3), target.size(4)).long()                           # [B*S, H, W]\n",
    "    cross_entropy = cross_entropy_loss(ce_logits, ce_target)\n",
    "\n",
    "    # Symmetry consistency penalty on features/logits (cosine mode by default)\n",
    "    symm_loss = symmetry_consistency_loss(feats, mode='cosine')\n",
    "\n",
    "    # Total loss as weighted sum\n",
    "    total_loss = dice_loss + cross_entropy + Î»_sym * symm_loss\n",
    "    \n",
    "    return total_loss, dice_scores, IoU, symm_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ff9df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model.to(device)                          # Move model to GPU/CPU\n",
    "# # model = nn.DataParallel(model)                  # (Optional) multi-GPU wrapper\n",
    "# optimizer = torch.optim.Adam(                     # Adam optimizer for all trainable params\n",
    "#     model.parameters(), lr=config.LR, weight_decay=1e-5\n",
    "# )\n",
    "\n",
    "# def choose_loader(epoch, total_epochs, verbose=True):\n",
    "#     \"\"\"\n",
    "#     Choose between mixed (random) loader and temporal neighbor loader.\n",
    "#     Phase schedule:\n",
    "#       - First T1 epochs â†’ MIX loader (higher diversity).\n",
    "#       - Remaining epochs â†’ no-MIX loader (temporal windows preserved).\n",
    "#     \"\"\"\n",
    "#     T1 = int(total_epochs * 1.0)                  # Here: always MIX for all epochs (as written)\n",
    "\n",
    "#     if epoch < T1:\n",
    "#         if verbose:\n",
    "#             print(f\"[Epoch {epoch:03d}] â€“ using MIX loader\")\n",
    "#         return train_loader_MIX                    # Phase-1 (diverse shuffling)\n",
    "#     else:\n",
    "#         if verbose:\n",
    "#             print(f\"[Epoch {epoch:03d}] â€“ using no-MIX loader\")\n",
    "#         return train_loader_noMIX                  # Phase-2+ (temporal windows)\n",
    "\n",
    "# # ---- Automatic Mixed Precision (AMP) setup ----\n",
    "# fp16_precision = True                              # Enable autocast for speed/memory\n",
    "# scaler = GradScaler(enabled=fp16_precision)        # Gradient scaler for AMP stability\n",
    "\n",
    "# # ---- Best checkpoint tracking ----\n",
    "# max_val_lesion_dice = 0\n",
    "# best_epoch = 0\n",
    "\n",
    "# # ---- Excel logging setup ----\n",
    "# summary_path = saveFile + \".xlsx\"\n",
    "# columns = ['Epoch', 'Train Loss', 'Val Loss', 'Train MYO Dice', 'Val MYO Dice', 'Train LV Dice', 'Val LV Dice']\n",
    "\n",
    "# if not os.path.exists(summary_path):\n",
    "#     wb = Workbook()\n",
    "#     ws = wb.active\n",
    "#     ws.append(columns)                             # Write header row\n",
    "#     wb.save(summary_path)\n",
    "\n",
    "# # ======================\n",
    "# # Training / Validation loop\n",
    "# # ======================\n",
    "# for epoch in range(config.num_epochs):\n",
    "#     # === Training ===\n",
    "#     model.train()\n",
    "#     train_loss = 0.0\n",
    "#     epoch_train_MYO_dice = 0.0\n",
    "#     epoch_train_LV_dice  = 0.0\n",
    "#     batch_count = 0\n",
    "\n",
    "#     loader = choose_loader(epoch, config.num_epochs, verbose=True)  # Select train loader phase\n",
    "#     train_loop = tqdm(loader, desc=f\"Train Epoch {epoch+1}/{config.num_epochs}\", leave=True)\n",
    "\n",
    "#     for batch_data in train_loop:\n",
    "#         batch_count += 1\n",
    "#         images   = batch_data['image'].to(device)     # [B,S,1,H,W]\n",
    "#         masks    = batch_data['mask'].to(device)      # [B,S,1,H,W] (integer labels per pixel)\n",
    "#         scribble = batch_data['scribble'].to(device)  # [B,S,1,H,W] (binary prompt)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         with autocast(enabled=fp16_precision):        # Forward in mixed precision\n",
    "#             predictions, features = model(images, scribble)   # predictions: [B,S,C,H,W], features: [B,S,Cb,Hf,Wf]\n",
    "#             loss, dice_scores, _, symm_losss = compute_loss(  # total loss + per-class Dice + symmetry loss\n",
    "#                 predictions, masks, features, device\n",
    "#             )\n",
    "\n",
    "#         # AMP backward & optimizer step\n",
    "#         scaler.scale(loss).backward()\n",
    "#         scaler.step(optimizer)\n",
    "#         scaler.update()\n",
    "\n",
    "#         # Accumulate metrics\n",
    "#         train_loss += loss.item()\n",
    "#         epoch_train_MYO_dice += dice_scores[1].item()  # assumes class-1 = MYO\n",
    "#         epoch_train_LV_dice  += dice_scores[2].item()  # assumes class-2 = LV\n",
    "\n",
    "#         # Live progress bar metrics\n",
    "#         train_loop.set_postfix({\n",
    "#             \"Loss\": f\"{loss.item():.4f}\",\n",
    "#             \"Symm_losss\": f\"{symm_losss.item():.4f}\",\n",
    "#             \"MYO Dice\": f\"{dice_scores[1]:.4f}\",\n",
    "#             \"LV Dice\": f\"{dice_scores[2]:.4f}\"\n",
    "#         })\n",
    "\n",
    "#     # === Validation ===\n",
    "#     model.eval()\n",
    "#     val_loss = 0.0\n",
    "#     epoch_val_MYO_dice = 0.0\n",
    "#     epoch_val_LV_dice  = 0.0\n",
    "#     val_batch_count = 0\n",
    "\n",
    "#     val_loop = tqdm(val_loader_MIX, desc=f\"Val Epoch {epoch+1}/{config.num_epochs}\", leave=True)\n",
    "#     with torch.no_grad():                            # Disable grad for validation\n",
    "#         for batch_data in val_loop:\n",
    "#             val_batch_count += 1\n",
    "#             images   = batch_data['image'].to(device)\n",
    "#             masks    = batch_data['mask'].to(device)\n",
    "#             scribble = batch_data['scribble'].to(device)\n",
    "\n",
    "#             with autocast(enabled=fp16_precision):\n",
    "#                 predictions, features = model(images, scribble)\n",
    "#                 loss, dice_scores, _, symm_losss = compute_loss(\n",
    "#                     predictions, masks, features, device\n",
    "#                 )\n",
    "\n",
    "#             # Accumulate metrics\n",
    "#             val_loss += loss.item()\n",
    "#             epoch_val_MYO_dice += dice_scores[1].item()\n",
    "#             epoch_val_LV_dice  += dice_scores[2].item()\n",
    "\n",
    "#             # Live progress bar metrics (val)\n",
    "#             val_loop.set_postfix({\n",
    "#                 \"Loss\": f\"{loss.item():.4f}\",\n",
    "#                 \"Symm_losss\": f\"{symm_losss.item():.4f}\",\n",
    "#                 \"MYO Dice\": f\"{dice_scores[1]:.4f}\",\n",
    "#                 \"LV Dice\": f\"{dice_scores[2]:.4f}\"\n",
    "#             })\n",
    "\n",
    "#     # === Epoch averages ===\n",
    "#     avg_train_loss   = train_loss / max(1, batch_count)\n",
    "#     avg_val_loss     = val_loss   / max(1, val_batch_count)\n",
    "#     avg_train_MYO_dice = epoch_train_MYO_dice / max(1, batch_count)\n",
    "#     avg_val_MYO_dice   = epoch_val_MYO_dice  / max(1, val_batch_count)\n",
    "#     avg_train_LV_dice  = epoch_train_LV_dice / max(1, batch_count)\n",
    "#     avg_val_LV_dice    = epoch_val_LV_dice   / max(1, val_batch_count)\n",
    "#     avg_val_dice       = (avg_val_MYO_dice + avg_val_LV_dice) / 2.0\n",
    "\n",
    "#     # Save best checkpoint by mean validation Dice (MYO & LV)\n",
    "#     if avg_val_dice > max_val_lesion_dice:\n",
    "#         max_val_lesion_dice = avg_val_dice\n",
    "#         best_epoch = epoch + 1\n",
    "#         torch.save(model.state_dict(), checkpoint_path)\n",
    "#         print(f\">>> Best model saved at Epoch {best_epoch} with Val Dice {avg_val_dice:.4f}\")\n",
    "\n",
    "#     print()  # newline after epoch logs\n",
    "\n",
    "#     # === Append epoch metrics to Excel ===\n",
    "#     wb = load_workbook(summary_path)\n",
    "#     ws = wb.active\n",
    "#     ws.append([\n",
    "#         epoch + 1,\n",
    "#         avg_train_loss,\n",
    "#         avg_val_loss,\n",
    "#         avg_train_MYO_dice,\n",
    "#         avg_val_MYO_dice,\n",
    "#         avg_train_LV_dice,\n",
    "#         avg_val_LV_dice\n",
    "#     ])\n",
    "#     wb.save(summary_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb3a355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the UNet model and move it to the selected device\n",
    "#   - num_classes: total classes in your segmentation (incl. background)\n",
    "#   - num_input_channels: input channels per slice (1 = grayscale)\n",
    "model = SCOPENet(\n",
    "    num_classes=config.num_classes,\n",
    "    num_input_channels=1\n",
    ")\n",
    "model = model.to(device)\n",
    "model.eval()  # Evaluation mode: disables Dropout/BatchNorm updates\n",
    "\n",
    "# Load trained weights from checkpoint\n",
    "# map_location ensures the weights load correctly on the current device\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "\n",
    "# Create output directory for saving predictions/visualizations\n",
    "if not os.path.exists(saveFile):\n",
    "    os.makedirs(saveFile)\n",
    "\n",
    "# Iterate over the test set\n",
    "for batch_data in tqdm(test_loader_MIX):\n",
    "\n",
    "    # Move batch tensors to device\n",
    "    images   = batch_data['image'].to(device)     # [B, 37, 1, H, W]\n",
    "    masks    = batch_data['mask'].to(device)      # [B, 37, 1, H, W]\n",
    "    scribble = batch_data['scribble'].to(device)  # [B, 37, 1, H, W]\n",
    "\n",
    "    # Forward pass (no grad needed, but eval() is already set)\n",
    "    predictions, _ = model(images, scribble)      # predictions: [B, 37, C, H, W]\n",
    "\n",
    "    # Convert probabilities/logits to hard labels via argmax over class dim (dim=2)\n",
    "    masks_predictions = torch.argmax(predictions, dim=2).unsqueeze(2).to(torch.float32)  # [B, 37, 1, H, W]\n",
    "\n",
    "    # Map class labels to visualization intensities for saving:\n",
    "    #   class 1 â†’ 200 (e.g., MYO), class 2 â†’ 100 (e.g., LV)\n",
    "    # NOTE: This changes the tensors in-place for saving images only.\n",
    "    masks_predictions[masks_predictions == 1] = 200\n",
    "    masks_predictions[masks_predictions == 2] = 100\n",
    "    masks[masks == 1] = 200\n",
    "    masks[masks == 2] = 100\n",
    "\n",
    "    # Save per-slice images/masks for each item in the batch\n",
    "    for slices in range(37):  # iterate over all radial slices\n",
    "        for batch in range(masks_predictions.shape[0]):  # iterate over batch items\n",
    "\n",
    "            # Extract numpy arrays for saving\n",
    "            true_mask = masks[batch, slices, :, :, :].reshape(config.img_size, config.img_size).detach().cpu().numpy()\n",
    "            pred_mask = masks_predictions[batch, slices, :, :, :].reshape(config.img_size, config.img_size).detach().cpu().numpy()\n",
    "            true_img  = images[batch, slices, :, :, :].reshape(config.img_size, config.img_size).detach().cpu().numpy()\n",
    "\n",
    "            # De-normalize image from [-1,1] back to [0,1] (assuming Normalize mean=0.5, std=0.5)\n",
    "            true_img = true_img * 0.5 + 0.5\n",
    "\n",
    "            # Build filenames from original paths:\n",
    "            #   - use the slice-level filename, strip '.png', then append suffixes\n",
    "            base_name = batch_data['name'][slices][0].split('/')[-1].replace('.png', '')\n",
    "\n",
    "            # Save predicted mask, ground-truth mask, and input image\n",
    "            cv2.imwrite(os.path.join(saveFile, f\"{base_name}_pred_mask.png\"), pred_mask)\n",
    "            cv2.imwrite(os.path.join(saveFile, f\"{base_name}_true_mask.png\"), true_mask)\n",
    "            cv2.imwrite(os.path.join(saveFile, f\"{base_name}_true_img.png\"), 255 * true_img)  # scale to 0â€“255\n",
    "\n",
    "            # Uncomment to debug a single item/slice:\n",
    "            # break\n",
    "        # break\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf62875e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
